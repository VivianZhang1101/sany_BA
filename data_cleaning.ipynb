{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4b9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695fa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sales_data(file_path):\n",
    "    \"\"\"\n",
    "    clean sales records\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the data\n",
    "    print(\"Reading data...\")\n",
    "    sales_df = pd.read_excel(file_path, sheet_name='Sales')\n",
    "    store_df = pd.read_excel(file_path, sheet_name='Store')\n",
    "    \n",
    "    print(f\"Sales data shape: {sales_df.shape}\")\n",
    "    print(f\"Sales columns: {list(sales_df.columns)}\")\n",
    "    print(f\"Store data shape: {store_df.shape}\")\n",
    "    print(f\"Store columns: {list(store_df.columns)}\")\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    cleaned_df = sales_df.copy()\n",
    "    \n",
    "    # 1. DATE CLEANING\n",
    "    # Clean SellDate (original format YYYYMMDD)\n",
    "    print(\"üîµ Cleaning date data...\")\n",
    "    def parse_mixed_date(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return None\n",
    "        # If float (e.g., 20230521.0), convert to int first\n",
    "        if isinstance(date_str, float) and date_str.is_integer():\n",
    "            date_str = str(int(date_str))\n",
    "        else:\n",
    "            date_str = str(date_str).strip()\n",
    "        # print(f\"Parsing date: {date_str}\")\n",
    "        # Only treat as YYYYMMDD if it's exactly 8 digits (like '20230521')\n",
    "        # errors='coerce' will convert invalid formats to NaT\n",
    "        if len(date_str) == 8 and date_str.isdigit():\n",
    "            # print(f\"Parsing date as YYYYMMDD: {date_str}\")\n",
    "            return pd.to_datetime(date_str, format='%Y%m%d', errors='coerce')\n",
    "        \n",
    "        # Otherwise let pandas infer the format (e.g., '9/3/2022')\n",
    "        return pd.to_datetime(date_str, errors='coerce')\n",
    "    \n",
    "    cleaned_df['SellDate'] = cleaned_df['SellDate'].apply(parse_mixed_date)\n",
    "    \n",
    "    # Clean ReceiveDate (original format M/D/YYYY)\n",
    "    cleaned_df['ReceiveDate'] = cleaned_df['ReceiveDate'].apply(parse_mixed_date)\n",
    "    # 2. FINANCIAL DATA CLEANING\n",
    "    print(\"üîµ Cleaning financial data...\")\n",
    "    try:\n",
    "        cleaned_df['Cost'] = (\n",
    "            cleaned_df['Cost']\n",
    "            .astype(str)              \n",
    "            .str.strip()                 \n",
    "            .replace(r'^$', np.nan, regex=True)\n",
    "            .replace('[\\$,]', '', regex=True)\n",
    "            .astype(float)\n",
    "        )\n",
    "        cleaned_df['Revenue'] = (\n",
    "            cleaned_df['Revenue']\n",
    "            .astype(str)              \n",
    "            .str.strip()                 \n",
    "            .replace(r'^$', np.nan, regex=True)\n",
    "            .replace('[\\$,]', '', regex=True)\n",
    "            .astype(float)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Financial data cleaning issue: {e}\")\n",
    "\n",
    "    \n",
    "    # Calculate derived financial metrics\n",
    "    cleaned_df['Profit'] = cleaned_df['Revenue'] - cleaned_df['Cost']\n",
    "    cleaned_df['ProfitMargin'] = np.where(\n",
    "        (cleaned_df['Revenue'] > 0) & (cleaned_df['Profit'].notna()),\n",
    "        (cleaned_df['Profit'] / cleaned_df['Revenue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    cleaned_df['ROI'] = np.where(\n",
    "        (cleaned_df['Cost'] > 0) & (cleaned_df['Profit'].notna()),\n",
    "        (cleaned_df['Profit'] / cleaned_df['Cost']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # 3. PRODUCT LINE AND WEIGHT EXTRACTION\n",
    "    print(\"üîµ Extracting product information...\")\n",
    "    \n",
    "    def extract_product_info(model_name):\n",
    "        \"\"\"Extract product line and weight from ModelName\"\"\"\n",
    "        if pd.isna(model_name):\n",
    "            return None, None\n",
    "        \n",
    "        # Use regex to extract letters (product line) and numbers (weight)\n",
    "        valid_product_lines = {'SY', 'SCA', 'TH'}\n",
    "        match = re.match(r'^([A-Z]+)(\\d+)', str(model_name).upper())\n",
    "        if match:\n",
    "            product_line = match.group(1)\n",
    "            weight = int(match.group(2))\n",
    "            # Validate product line\n",
    "            if product_line in valid_product_lines:\n",
    "                return product_line, weight\n",
    "            else:\n",
    "                # Invalid product line - return None for both\n",
    "                return None, None\n",
    "        return None, None\n",
    "    \n",
    "    # Extract product line and weight\n",
    "    product_info = cleaned_df['ModelName'].apply(extract_product_info)\n",
    "    cleaned_df['ProductLine'] = [info[0] for info in product_info]\n",
    "    cleaned_df['Weight'] = [info[1] for info in product_info]\n",
    "    \n",
    "    # 4. WEIGHT CLASS CATEGORIZATION\n",
    "    print(\"üîµ Categorizing weight classes...\")\n",
    "    \n",
    "    def categorize_weight(weight):\n",
    "        \"\"\"Categorize products by weight class\"\"\"\n",
    "        if pd.isna(weight):\n",
    "            return None\n",
    "        try:\n",
    "            weight = float(weight)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "        if weight <= 35:\n",
    "            return 'Small'\n",
    "        elif weight < 155:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Large'\n",
    "    \n",
    "    cleaned_df['WeightClass'] = cleaned_df['Weight'].apply(categorize_weight)\n",
    "    \n",
    "    # 5. LOCATION PARSING\n",
    "    print(\"üîµ Parsing location data...\")\n",
    "    \n",
    "    def parse_location(location):\n",
    "        \"\"\"Parse location into county and state\"\"\"\n",
    "        if pd.isna(location):\n",
    "            return None, None\n",
    "        \n",
    "        parts = str(location).split(',')\n",
    "        if len(parts) >= 2:\n",
    "            county = parts[0].strip()\n",
    "            state = parts[1].strip()\n",
    "            return county, state\n",
    "        return location.strip(), None\n",
    "    \n",
    "    location_info = cleaned_df['SaleLocation'].apply(parse_location)\n",
    "    cleaned_df['City'] = [info[0] for info in location_info]\n",
    "    cleaned_df['State'] = [info[1] for info in location_info]\n",
    "    \n",
    "    # 6. MERGE WITH STORE DATA\n",
    "    print(\"üîµ Merging with store information...\")\n",
    "    initial_row_count = len(cleaned_df)\n",
    "    # Merge store information\n",
    "    cleaned_df = cleaned_df.merge(store_df, on='StoreID', how='left')\n",
    "    print(f\"  Store data merged. Records with store info: {cleaned_df['StoreName'].notna().sum()}\")\n",
    "    # Validate the merge\n",
    "    final_row_count = len(cleaned_df)\n",
    "    rows_without_store_info = cleaned_df['StoreName'].isna().sum()\n",
    "\n",
    "    # Show which StoreIDs don't have matches\n",
    "    if rows_without_store_info > 0:\n",
    "        print(\"  ‚ö†Ô∏è  StoreIDs without matching store information:\")\n",
    "        unmatched_ids = cleaned_df[cleaned_df['StoreName'].isna()]['StoreID'].unique()\n",
    "        for store_id in sorted(unmatched_ids)[:10]:  # Show first 10\n",
    "            count = (cleaned_df['StoreID'] == store_id).sum()\n",
    "            print(f\"    StoreID '{store_id}': {count} records\")\n",
    "        if len(unmatched_ids) > 10:\n",
    "            print(f\"    ... and {len(unmatched_ids) - 10} more StoreIDs\")\n",
    "\n",
    "    # Validation check\n",
    "    if final_row_count != initial_row_count:\n",
    "        print(\"  ‚ö†Ô∏è  WARNING: Row count changed during merge! This shouldn't happen with left join.\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Merge completed successfully - no rows lost\")\n",
    "    # 7. INVENTORY METRICS\n",
    "    print(\"üîµ Calculating inventory metrics...\")\n",
    "    \n",
    "    # Calculate days in inventory\n",
    "    cleaned_df['InventoryDays'] = (\n",
    "        cleaned_df['SellDate'] - cleaned_df['ReceiveDate']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Handle negative inventory days (data quality issue)\n",
    "    cleaned_df['InventoryDays'] = np.where(\n",
    "        cleaned_df['InventoryDays'] < 0, \n",
    "        np.nan, \n",
    "        cleaned_df['InventoryDays']\n",
    "    )\n",
    "    \n",
    "    # 8. DATA TYPE CONVERSIONS\n",
    "    print(\"üîµ Converting data types...\")\n",
    "    \n",
    "    cleaned_df['StoreID'] = pd.to_numeric(cleaned_df['StoreID'], errors='coerce')\n",
    "    cleaned_df['SerialNumber'] = cleaned_df['SerialNumber'].astype(str)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    # 9. Check for duplicate serial numbers rows\n",
    "    print(\"üîµ Checking for duplicate rows...\")\n",
    "    key_columns = ['SellDate', 'SerialNumber', 'ModelName', 'ReceiveDate', 'Cost', 'Revenue', 'StoreID', 'SaleLocation', 'Usage']\n",
    "\n",
    "    # Step 1: Identify all rows with duplicate serial numbers\n",
    "    has_duplicate_serial = cleaned_df.duplicated(subset=['SerialNumber'], keep=False)\n",
    "    total_rows_with_dup_serials = has_duplicate_serial.sum()\n",
    "\n",
    "    # Step 2: Identify exact duplicates (same across all key columns) - mark ALL instances\n",
    "    all_exact_duplicates = cleaned_df.duplicated(subset=key_columns, keep=False)\n",
    "\n",
    "    # Step 3: Exact duplicates to remove (keep first)\n",
    "    exact_duplicates_to_remove = cleaned_df.duplicated(subset=key_columns, keep='first')\n",
    "    num_exact_duplicates = exact_duplicates_to_remove.sum()\n",
    "\n",
    "    # Step 4: TRUE conflicting duplicates = rows with duplicate serials that are NOT part of exact duplicate sets\n",
    "    conflicting_duplicates_mask = has_duplicate_serial & ~all_exact_duplicates\n",
    "    num_conflicting_rows = conflicting_duplicates_mask.sum()\n",
    "\n",
    "    # Export for review\n",
    "    exact_duplicate_rows = cleaned_df[exact_duplicates_to_remove]\n",
    "    exact_duplicate_rows.to_csv(\"exact_duplicates_to_remove.csv\", index=False)\n",
    "\n",
    "    conflicting_duplicate_rows = cleaned_df[conflicting_duplicates_mask]\n",
    "    conflicting_duplicate_rows.to_csv(\"conflicting_duplicates.csv\", index=False)\n",
    "\n",
    "    print(\"  üîÑ Exported exact and conflicting duplicates for review.\")\n",
    "\n",
    "    # Drop exact duplicates\n",
    "    cleaned_df = cleaned_df[~exact_duplicates_to_remove]\n",
    "\n",
    "    # Summary statistics\n",
    "    unique_serial_numbers_with_conflicts = cleaned_df[cleaned_df.duplicated(subset=['SerialNumber'], keep=False)]['SerialNumber'].nunique()\n",
    "\n",
    "    print(f\"  üìä DUPLICATE ANALYSIS:\")\n",
    "    print(f\"     ‚Ä¢ Total rows with duplicate serial numbers: {total_rows_with_dup_serials}\")\n",
    "    print(f\"     ‚Ä¢ Rows in exact duplicate sets (all instances): {all_exact_duplicates.sum()}\")\n",
    "    print(f\"     ‚Ä¢ Exact duplicate rows removed (keeping first): {num_exact_duplicates}\")\n",
    "    print(f\"     ‚Ä¢ TRUE conflicting duplicate rows: {num_conflicting_rows}\")\n",
    "    print(f\"     ‚Ä¢ Unique serial numbers with conflicts: {unique_serial_numbers_with_conflicts}\")\n",
    "    print(f\"     ‚Ä¢ Rows remaining after cleanup: {len(cleaned_df)}\")\n",
    "\n",
    "    # Additional validation\n",
    "    total_exact_duplicate_instances = all_exact_duplicates.sum()\n",
    "    expected_total = total_exact_duplicate_instances + num_conflicting_rows\n",
    "\n",
    "    if expected_total != total_rows_with_dup_serials:\n",
    "        print(\"  ‚ö†Ô∏è  WARNING: Duplicate counts don't add up correctly!\")\n",
    "        print(f\"     Expected: {expected_total}, Actual: {total_rows_with_dup_serials}\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Duplicate counts validated successfully\")\n",
    "\n",
    "    # 10. CREATE ADDITIONAL ANALYTICAL COLUMNS\n",
    "    print(\"\\nüîµ Creating additional analytical columns...\")\n",
    "    \n",
    "    # Revenue quartiles\n",
    "    cleaned_df['RevenueQuartile'] = pd.qcut(\n",
    "        cleaned_df['Revenue'], \n",
    "        q=4, \n",
    "        labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 11. DATA QUALITY CHECKS\n",
    "    print(\"üîµ Data quality summary:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = cleaned_df.isnull().sum()\n",
    "    print(\"  üìä Missing values per column:\")\n",
    "    for col, missing in missing_values.items():\n",
    "        if missing > 0:\n",
    "            print(f\"    {col}: {missing} ({missing/len(cleaned_df)*100:.1f}%)\")\n",
    "          \n",
    "    # rows with any missing or unknown values\n",
    "    rows_with_null = cleaned_df.isnull().any(axis=1)\n",
    "    percent_with_any_missing = rows_with_null / len(cleaned_df) * 100\n",
    "    print(f\"  üìä Percentage of Rows with null values: {rows_with_null.sum()} ({rows_with_null.sum()/len(cleaned_df)*100:.1f}%)\")\n",
    "\n",
    "    \n",
    "    # Check for negative financial values\n",
    "    negative_revenue = (cleaned_df['Revenue'] < 0).sum()\n",
    "    negative_cost = (cleaned_df['Cost'] < 0).sum()\n",
    "    negative_profit = (cleaned_df['Profit'] < 0).sum()\n",
    "    print(f\"  üî¥ Negative revenue records: {negative_revenue}\")\n",
    "    print(f\"  üî¥ Negative cost records: {negative_cost}\")\n",
    "    print(f\"  üî¥ Negative profit records: {negative_profit}\\n\")\n",
    "    \n",
    "    # 11. FINAL DATASET ORGANIZATION\n",
    "    print(\"üîµ Organizing final dataset...\")\n",
    "    \n",
    "    # Reorder columns for better organization\n",
    "    column_order = [\n",
    "        # Identifiers\n",
    "        'SerialNumber', 'ModelName', 'ProductLine', 'Weight', 'WeightClass',\n",
    "        \n",
    "        # Dates\n",
    "        'SellDate', 'ReceiveDate', 'InventoryDays',\n",
    "        \n",
    "        # Financial\n",
    "        'Cost', 'Revenue', 'Profit', 'ProfitMargin', 'ROI', 'RevenueQuartile',\n",
    "        \n",
    "        # Location & Store\n",
    "        'StoreID', 'StoreName', 'State', 'City', 'SaleLocation',\n",
    "        \n",
    "        # Customer\n",
    "        'Usage'\n",
    "    ]\n",
    "    \n",
    "    # Include only columns that exist in the dataframe\n",
    "    final_columns = [col for col in column_order if col in cleaned_df.columns]\n",
    "    cleaned_df = cleaned_df[final_columns]\n",
    "    \n",
    "    print(f\"Final dataset shape: {cleaned_df.shape}\")\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a87cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_summary(df):\n",
    "    \"\"\"Generate comprehensive data summary for analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîµ DATA SUMMARY FOR TABLEAU ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüîµ Dataset Overview:\")\n",
    "    print(f\"Total Records: {len(df):,}\")\n",
    "    print(f\"Date Range: {df['SellDate'].min()} to {df['SellDate'].max()}\")\n",
    "    print(f\"Total Revenue: ${df['Revenue'].sum():,.2f}\")\n",
    "    print(f\"Total Profit: ${df['Profit'].sum():,.2f}\")\n",
    "    print(f\"Average Profit Margin: {df['ProfitMargin'].mean():.2f}%\")\n",
    "    \n",
    "    # Product line analysis\n",
    "    print(f\"\\nüè≠ Product Line Breakdown:\")\n",
    "    product_summary = df.groupby('ProductLine').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(product_summary)\n",
    "    \n",
    "    # Weight class analysis\n",
    "    print(f\"\\n‚öñÔ∏è Weight Class Distribution:\")\n",
    "    weight_summary = df.groupby('WeightClass').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(weight_summary)\n",
    "    # # Store performance analysis\n",
    "    print(f\"\\nüè™ Store Performance:\")\n",
    "    store_summary = df.groupby(['StoreID', 'StoreName']).agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(store_summary)\n",
    "    # # Geographic distribution\n",
    "    print(f\"\\nüó∫Ô∏è Top 10 States by Revenue:\")\n",
    "    state_summary = df.groupby('State').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2).sort_values(('Revenue', 'sum'), ascending=False).head(10)\n",
    "\n",
    "    print(state_summary)\n",
    "    # # Usage category analysis\n",
    "    print(f\"\\nüë• Usage Category Performance:\")\n",
    "    usage_summary = df.groupby('Usage').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(usage_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81370be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = 'BA_DummyData_2025.xlsx' \n",
    "    \n",
    "    try:\n",
    "        # Clean the data\n",
    "        cleaned_data = clean_sales_data(file_path)\n",
    "        \n",
    "        # Generate summary\n",
    "        generate_data_summary(cleaned_data)\n",
    "        \n",
    "        # Save cleaned data\n",
    "        output_file = 'cleaned_BA_DummyData_2025.csv'\n",
    "        cleaned_data.to_csv(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Cleaned data saved to: {output_file}\")\n",
    "        \n",
    "        return cleaned_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        print(\"Please update the file_path variable with the correct path to your CSV file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fbb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # # Inventory performance categories\n",
    "    # def categorize_inventory_performance(days):\n",
    "    #     if pd.isna(days):\n",
    "    #         return 'Unknown'\n",
    "    #     elif days <= 90:\n",
    "    #         return 'Fast'\n",
    "    #     elif days <= 180:\n",
    "    #         return 'Normal'\n",
    "    #     elif days <= 365:\n",
    "    #         return 'Slow'\n",
    "    #     else:\n",
    "    #         return 'Very Slow'\n",
    "    \n",
    "    # cleaned_df['InventoryPerformance'] = cleaned_df['InventoryDays'].apply(\n",
    "    #     categorize_inventory_performance\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ab738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sany_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
