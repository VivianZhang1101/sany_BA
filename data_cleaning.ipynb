{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4b9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695fa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sales_data(file_path):\n",
    "    \"\"\"\n",
    "    clean sales records\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the data\n",
    "    print(\"Reading data...\")\n",
    "    sales_df = pd.read_excel(file_path, sheet_name='Sales')\n",
    "    store_df = pd.read_excel(file_path, sheet_name='Store')\n",
    "    \n",
    "    print(f\"Sales data shape: {sales_df.shape}\")\n",
    "    print(f\"Sales columns: {list(sales_df.columns)}\")\n",
    "    print(f\"Store data shape: {store_df.shape}\")\n",
    "    print(f\"Store columns: {list(store_df.columns)}\")\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    cleaned_df = sales_df.copy()\n",
    "    \n",
    "    # 1. DATE CLEANING\n",
    "    # Clean SellDate (original format YYYYMMDD)\n",
    "    print(\"üîµ Cleaning date data...\")\n",
    "    def parse_mixed_date(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return None\n",
    "        # If float (e.g., 20230521.0), convert to int first\n",
    "        if isinstance(date_str, float) and date_str.is_integer():\n",
    "            date_str = str(int(date_str))\n",
    "        else:\n",
    "            date_str = str(date_str).strip()\n",
    "        # print(f\"Parsing date: {date_str}\")\n",
    "        # Only treat as YYYYMMDD if it's exactly 8 digits (like '20230521')\n",
    "        # errors='coerce' will convert invalid formats to NaT\n",
    "        if len(date_str) == 8 and date_str.isdigit():\n",
    "            # print(f\"Parsing date as YYYYMMDD: {date_str}\")\n",
    "            return pd.to_datetime(date_str, format='%Y%m%d', errors='coerce')\n",
    "        \n",
    "        # Otherwise let pandas infer the format (e.g., '9/3/2022')\n",
    "        return pd.to_datetime(date_str, errors='coerce')\n",
    "    \n",
    "    cleaned_df['SellDate'] = cleaned_df['SellDate'].apply(parse_mixed_date)\n",
    "    \n",
    "    # Clean ReceiveDate (original format M/D/YYYY)\n",
    "    cleaned_df['ReceiveDate'] = cleaned_df['ReceiveDate'].apply(parse_mixed_date)\n",
    "    # 2. FINANCIAL DATA CLEANING\n",
    "    print(\"üîµ Cleaning financial data...\")\n",
    "    try:\n",
    "        cleaned_df['Cost'] = (\n",
    "            cleaned_df['Cost']\n",
    "            .astype(str)              \n",
    "            .str.strip()                 \n",
    "            .replace(r'^$', np.nan, regex=True)\n",
    "            .replace('[\\$,]', '', regex=True)\n",
    "            .astype(float)\n",
    "        )\n",
    "        cleaned_df['Revenue'] = (\n",
    "            cleaned_df['Revenue']\n",
    "            .astype(str)              \n",
    "            .str.strip()                 \n",
    "            .replace(r'^$', np.nan, regex=True)\n",
    "            .replace('[\\$,]', '', regex=True)\n",
    "            .astype(float)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Financial data cleaning issue: {e}\")\n",
    "\n",
    "    \n",
    "    # Calculate derived financial metrics\n",
    "    cleaned_df['Profit'] = cleaned_df['Revenue'] - cleaned_df['Cost']\n",
    "    cleaned_df['ProfitMargin'] = np.where(\n",
    "        (cleaned_df['Revenue'] > 0) & (cleaned_df['Profit'].notna()),\n",
    "        (cleaned_df['Profit'] / cleaned_df['Revenue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    cleaned_df['ROI'] = np.where(\n",
    "        (cleaned_df['Cost'] > 0) & (cleaned_df['Profit'].notna()),\n",
    "        (cleaned_df['Profit'] / cleaned_df['Cost']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # 3. PRODUCT LINE AND WEIGHT EXTRACTION\n",
    "    print(\"üîµ Extracting product information...\")\n",
    "    \n",
    "    def extract_product_info(model_name):\n",
    "        \"\"\"Extract product line and weight from ModelName\"\"\"\n",
    "        if pd.isna(model_name):\n",
    "            return None, None\n",
    "        \n",
    "        # Use regex to extract letters (product line) and numbers (weight)\n",
    "        valid_product_lines = {'SY', 'SCA', 'TH'}\n",
    "        match = re.match(r'^([A-Z]+)(\\d+)', str(model_name).upper())\n",
    "        if match:\n",
    "            product_line = match.group(1)\n",
    "            weight = int(match.group(2))\n",
    "            # Validate product line\n",
    "            if product_line in valid_product_lines:\n",
    "                return product_line, weight\n",
    "            else:\n",
    "                # Invalid product line - return None for both\n",
    "                return None, None\n",
    "        return None, None\n",
    "    \n",
    "    # Extract product line and weight\n",
    "    product_info = cleaned_df['ModelName'].apply(extract_product_info)\n",
    "    cleaned_df['ProductLine'] = [info[0] for info in product_info]\n",
    "    cleaned_df['Weight'] = [info[1] for info in product_info]\n",
    "    \n",
    "    # 4. WEIGHT CLASS CATEGORIZATION\n",
    "    print(\"üîµ Categorizing weight classes...\")\n",
    "    \n",
    "    def categorize_weight(weight):\n",
    "        \"\"\"Categorize products by weight class\"\"\"\n",
    "        if pd.isna(weight):\n",
    "            return None\n",
    "        try:\n",
    "            weight = float(weight)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "        if weight <= 35:\n",
    "            return 'Small'\n",
    "        elif weight < 155:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Large'\n",
    "    \n",
    "    cleaned_df['WeightClass'] = cleaned_df['Weight'].apply(categorize_weight)\n",
    "    \n",
    "    # 5. LOCATION PARSING\n",
    "    print(\"üîµ Parsing location data...\")\n",
    "    \n",
    "    def parse_location(location):\n",
    "        \"\"\"Parse location into county and state\"\"\"\n",
    "        if pd.isna(location):\n",
    "            return None, None\n",
    "        \n",
    "        parts = str(location).split(',')\n",
    "        if len(parts) >= 2:\n",
    "            county = parts[0].strip()\n",
    "            state = parts[1].strip()\n",
    "            return county, state\n",
    "        return location.strip(), None\n",
    "    \n",
    "    location_info = cleaned_df['SaleLocation'].apply(parse_location)\n",
    "    cleaned_df['City'] = [info[0] for info in location_info]\n",
    "    cleaned_df['State'] = [info[1] for info in location_info]\n",
    "    \n",
    "    # 6. MERGE WITH STORE DATA\n",
    "    print(\"üîµ Merging with store information...\")\n",
    "    initial_row_count = len(cleaned_df)\n",
    "    # Merge store information\n",
    "    cleaned_df = cleaned_df.merge(store_df, on='StoreID', how='left')\n",
    "    print(f\"  Store data merged. Records with store info: {cleaned_df['StoreName'].notna().sum()}\")\n",
    "    # Validate the merge\n",
    "    final_row_count = len(cleaned_df)\n",
    "    rows_without_store_info = cleaned_df['StoreName'].isna().sum()\n",
    "\n",
    "    # Show which StoreIDs don't have matches\n",
    "    if rows_without_store_info > 0:\n",
    "        print(\"  ‚ö†Ô∏è  StoreIDs without matching store information:\")\n",
    "        unmatched_ids = cleaned_df[cleaned_df['StoreName'].isna()]['StoreID'].unique()\n",
    "        for store_id in sorted(unmatched_ids)[:10]:  # Show first 10\n",
    "            count = (cleaned_df['StoreID'] == store_id).sum()\n",
    "            print(f\"    StoreID '{store_id}': {count} records\")\n",
    "        if len(unmatched_ids) > 10:\n",
    "            print(f\"    ... and {len(unmatched_ids) - 10} more StoreIDs\")\n",
    "\n",
    "    # Validation check\n",
    "    if final_row_count != initial_row_count:\n",
    "        print(\"  ‚ö†Ô∏è  WARNING: Row count changed during merge! This shouldn't happen with left join.\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Merge completed successfully - no rows lost\")\n",
    "    # 7. INVENTORY METRICS\n",
    "    print(\"üîµ Calculating inventory metrics...\")\n",
    "    \n",
    "    # Calculate days in inventory\n",
    "    cleaned_df['InventoryDays'] = (\n",
    "        cleaned_df['SellDate'] - cleaned_df['ReceiveDate']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Handle negative inventory days (data quality issue)\n",
    "    cleaned_df['InventoryDays'] = np.where(\n",
    "        cleaned_df['InventoryDays'] < 0, \n",
    "        np.nan, \n",
    "        cleaned_df['InventoryDays']\n",
    "    )\n",
    "    \n",
    "    # 8. DATA TYPE CONVERSIONS\n",
    "    print(\"üîµ Converting data types...\")\n",
    "    \n",
    "    cleaned_df['StoreID'] = pd.to_numeric(cleaned_df['StoreID'], errors='coerce')\n",
    "    cleaned_df['SerialNumber'] = cleaned_df['SerialNumber'].astype(str)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    # 9. Check for duplicate serial numbers rows\n",
    "    print(\"üîµ Checking for duplicate rows...\")\n",
    "    key_columns = ['SellDate', 'SerialNumber', 'ModelName', 'ReceiveDate', 'Cost', 'Revenue', 'StoreID', 'SaleLocation', 'Usage']\n",
    "\n",
    "    # Step 1: Identify all rows with duplicate serial numbers\n",
    "    has_duplicate_serial = cleaned_df.duplicated(subset=['SerialNumber'], keep=False)\n",
    "    total_rows_with_dup_serials = has_duplicate_serial.sum()\n",
    "\n",
    "    # Step 2: Identify exact duplicates (same across all key columns) - mark ALL instances\n",
    "    all_exact_duplicates = cleaned_df.duplicated(subset=key_columns, keep=False)\n",
    "\n",
    "    # Step 3: Exact duplicates to remove (keep first)\n",
    "    exact_duplicates_to_remove = cleaned_df.duplicated(subset=key_columns, keep='first')\n",
    "    num_exact_duplicates = exact_duplicates_to_remove.sum()\n",
    "\n",
    "    # Step 4: TRUE conflicting duplicates = rows with duplicate serials that are NOT part of exact duplicate sets\n",
    "    conflicting_duplicates_mask = has_duplicate_serial & ~all_exact_duplicates\n",
    "    num_conflicting_rows = conflicting_duplicates_mask.sum()\n",
    "\n",
    "    # Export for review\n",
    "    exact_duplicate_rows = cleaned_df[exact_duplicates_to_remove]\n",
    "    exact_duplicate_rows.to_csv(\"exact_duplicates_to_remove.csv\", index=False)\n",
    "\n",
    "    conflicting_duplicate_rows = cleaned_df[conflicting_duplicates_mask]\n",
    "    conflicting_duplicate_rows.to_csv(\"conflicting_duplicates.csv\", index=False)\n",
    "\n",
    "    print(\"  üîÑ Exported exact and conflicting duplicates for review.\")\n",
    "\n",
    "    # Drop exact duplicates\n",
    "    cleaned_df = cleaned_df[~exact_duplicates_to_remove]\n",
    "\n",
    "    # Summary statistics\n",
    "    unique_serial_numbers_with_conflicts = cleaned_df[cleaned_df.duplicated(subset=['SerialNumber'], keep=False)]['SerialNumber'].nunique()\n",
    "\n",
    "    print(f\"  üìä DUPLICATE ANALYSIS:\")\n",
    "    print(f\"     ‚Ä¢ Total rows with duplicate serial numbers: {total_rows_with_dup_serials}\")\n",
    "    print(f\"     ‚Ä¢ Rows in exact duplicate sets (all instances): {all_exact_duplicates.sum()}\")\n",
    "    print(f\"     ‚Ä¢ Exact duplicate rows removed (keeping first): {num_exact_duplicates}\")\n",
    "    print(f\"     ‚Ä¢ TRUE conflicting duplicate rows: {num_conflicting_rows}\")\n",
    "    print(f\"     ‚Ä¢ Unique serial numbers with conflicts: {unique_serial_numbers_with_conflicts}\")\n",
    "    print(f\"     ‚Ä¢ Rows remaining after cleanup: {len(cleaned_df)}\")\n",
    "\n",
    "    # Additional validation\n",
    "    total_exact_duplicate_instances = all_exact_duplicates.sum()\n",
    "    expected_total = total_exact_duplicate_instances + num_conflicting_rows\n",
    "\n",
    "    if expected_total != total_rows_with_dup_serials:\n",
    "        print(\"  ‚ö†Ô∏è  WARNING: Duplicate counts don't add up correctly!\")\n",
    "        print(f\"     Expected: {expected_total}, Actual: {total_rows_with_dup_serials}\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Duplicate counts validated successfully\")\n",
    "\n",
    "    # 10. CREATE ADDITIONAL ANALYTICAL COLUMNS\n",
    "    print(\"\\nüîµ Creating additional analytical columns...\")\n",
    "    \n",
    "    # Revenue quartiles\n",
    "    cleaned_df['RevenueQuartile'] = pd.qcut(\n",
    "        cleaned_df['Revenue'], \n",
    "        q=4, \n",
    "        labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 11. DATA QUALITY CHECKS\n",
    "    print(\"üîµ Data quality summary:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = cleaned_df.isnull().sum()\n",
    "    print(\"  üìä Missing values per column:\")\n",
    "    for col, missing in missing_values.items():\n",
    "        if missing > 0:\n",
    "            print(f\"    {col}: {missing} ({missing/len(cleaned_df)*100:.1f}%)\")\n",
    "          \n",
    "    # rows with any missing or unknown values\n",
    "    rows_with_null = cleaned_df.isnull().any(axis=1)\n",
    "    percent_with_any_missing = rows_with_null / len(cleaned_df) * 100\n",
    "    print(f\"  üìä Percentage of Rows with null values: {rows_with_null.sum()} ({rows_with_null.sum()/len(cleaned_df)*100:.1f}%)\")\n",
    "\n",
    "    \n",
    "    # Check for negative financial values\n",
    "    negative_revenue = (cleaned_df['Revenue'] < 0).sum()\n",
    "    negative_cost = (cleaned_df['Cost'] < 0).sum()\n",
    "    negative_profit = (cleaned_df['Profit'] < 0).sum()\n",
    "    print(f\"  üî¥ Negative revenue records: {negative_revenue}\")\n",
    "    print(f\"  üî¥ Negative cost records: {negative_cost}\")\n",
    "    print(f\"  üî¥ Negative profit records: {negative_profit}\\n\")\n",
    "    \n",
    "    # 11. FINAL DATASET ORGANIZATION\n",
    "    print(\"üîµ Organizing final dataset...\")\n",
    "    \n",
    "    # Reorder columns for better organization\n",
    "    column_order = [\n",
    "        # Identifiers\n",
    "        'SerialNumber', 'ModelName', 'ProductLine', 'Weight', 'WeightClass',\n",
    "        \n",
    "        # Dates\n",
    "        'SellDate', 'ReceiveDate', 'InventoryDays',\n",
    "        \n",
    "        # Financial\n",
    "        'Cost', 'Revenue', 'Profit', 'ProfitMargin', 'ROI', 'RevenueQuartile',\n",
    "        \n",
    "        # Location & Store\n",
    "        'StoreID', 'StoreName', 'State', 'City', 'SaleLocation',\n",
    "        \n",
    "        # Customer\n",
    "        'Usage'\n",
    "    ]\n",
    "    \n",
    "    # Include only columns that exist in the dataframe\n",
    "    final_columns = [col for col in column_order if col in cleaned_df.columns]\n",
    "    cleaned_df = cleaned_df[final_columns]\n",
    "    \n",
    "    print(f\"Final dataset shape: {cleaned_df.shape}\")\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a87cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_summary(df):\n",
    "    \"\"\"Generate comprehensive data summary for analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîµ DATA SUMMARY FOR TABLEAU ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüîµ Dataset Overview:\")\n",
    "    print(f\"Total Records: {len(df):,}\")\n",
    "    print(f\"Date Range: {df['SellDate'].min()} to {df['SellDate'].max()}\")\n",
    "    print(f\"Total Revenue: ${df['Revenue'].sum():,.2f}\")\n",
    "    print(f\"Total Profit: ${df['Profit'].sum():,.2f}\")\n",
    "    print(f\"Average Profit Margin: {df['ProfitMargin'].mean():.2f}%\")\n",
    "    \n",
    "    # Product line analysis\n",
    "    print(f\"\\nüè≠ Product Line Breakdown:\")\n",
    "    product_summary = df.groupby('ProductLine').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(product_summary)\n",
    "    \n",
    "    # Weight class analysis\n",
    "    print(f\"\\n‚öñÔ∏è Weight Class Distribution:\")\n",
    "    weight_summary = df.groupby('WeightClass').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(weight_summary)\n",
    "    # # Store performance analysis\n",
    "    print(f\"\\nüè™ Store Performance:\")\n",
    "    store_summary = df.groupby(['StoreID', 'StoreName']).agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(store_summary)\n",
    "    # # Geographic distribution\n",
    "    print(f\"\\nüó∫Ô∏è Top 10 States by Revenue:\")\n",
    "    state_summary = df.groupby('State').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2).sort_values(('Revenue', 'sum'), ascending=False).head(10)\n",
    "\n",
    "    print(state_summary)\n",
    "    # # Usage category analysis\n",
    "    print(f\"\\nüë• Usage Category Performance:\")\n",
    "    usage_summary = df.groupby('Usage').agg({\n",
    "        'Revenue': ['count', 'sum', 'mean'],\n",
    "        'Profit': ['sum','mean'],\n",
    "        'ProfitMargin': 'mean'\n",
    "    }).round(2)\n",
    "    print(usage_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81370be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = 'BA_DummyData_2025.xlsx' \n",
    "    \n",
    "    try:\n",
    "        # Clean the data\n",
    "        cleaned_data = clean_sales_data(file_path)\n",
    "        \n",
    "        # Generate summary\n",
    "        generate_data_summary(cleaned_data)\n",
    "        \n",
    "        # Save cleaned data\n",
    "        output_file = 'cleaned_BA_DummyData_2025.csv'\n",
    "        cleaned_data.to_csv(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Cleaned data saved to: {output_file}\")\n",
    "        \n",
    "        return cleaned_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        print(\"Please update the file_path variable with the correct path to your CSV file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Sales data shape: (6050, 9)\n",
      "Sales columns: ['SellDate', 'SerialNumber', 'ModelName', 'ReceiveDate', 'Cost', 'Revenue', 'StoreID', 'SaleLocation', 'Usage']\n",
      "Store data shape: (6, 2)\n",
      "Store columns: ['StoreID', 'StoreName']\n",
      "üîµ Cleaning date data...\n",
      "üîµ Cleaning financial data...\n",
      "üîµ Extracting product information...\n",
      "üîµ Categorizing weight classes...\n",
      "üîµ Parsing location data...\n",
      "üîµ Merging with store information...\n",
      "  Store data merged. Records with store info: 6050\n",
      "  ‚úÖ Merge completed successfully - no rows lost\n",
      "üîµ Calculating inventory metrics...\n",
      "üîµ Converting data types...\n",
      "üîµ Checking for duplicate rows...\n",
      "  üîÑ Exported exact and conflicting duplicates for review.\n",
      "  üìä DUPLICATE ANALYSIS:\n",
      "     ‚Ä¢ Total rows with duplicate serial numbers: 100\n",
      "     ‚Ä¢ Rows in exact duplicate sets (all instances): 100\n",
      "     ‚Ä¢ Exact duplicate rows removed (keeping first): 50\n",
      "     ‚Ä¢ TRUE conflicting duplicate rows: 0\n",
      "     ‚Ä¢ Unique serial numbers with conflicts: 0\n",
      "     ‚Ä¢ Rows remaining after cleanup: 6000\n",
      "  ‚úÖ Duplicate counts validated successfully\n",
      "\n",
      "üîµ Creating additional analytical columns...\n",
      "üîµ Data quality summary:\n",
      "  üìä Missing values per column:\n",
      "    SellDate: 863 (14.4%)\n",
      "    Cost: 50 (0.8%)\n",
      "    Revenue: 863 (14.4%)\n",
      "    SaleLocation: 863 (14.4%)\n",
      "    Usage: 863 (14.4%)\n",
      "    Profit: 913 (15.2%)\n",
      "    ProfitMargin: 1004 (16.7%)\n",
      "    ROI: 913 (15.2%)\n",
      "    City: 863 (14.4%)\n",
      "    State: 863 (14.4%)\n",
      "    InventoryDays: 863 (14.4%)\n",
      "    RevenueQuartile: 863 (14.4%)\n",
      "  üìä Percentage of Rows with null values: 1004 (16.7%)\n",
      "  üî¥ Negative revenue records: 0\n",
      "  üî¥ Negative cost records: 0\n",
      "  üî¥ Negative profit records: 146\n",
      "\n",
      "üîµ Organizing final dataset...\n",
      "Final dataset shape: (6000, 20)\n",
      "\n",
      "============================================================\n",
      "üîµ DATA SUMMARY FOR TABLEAU ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üîµ Dataset Overview:\n",
      "Total Records: 6,000\n",
      "Date Range: 2023-01-10 00:00:00 to 2024-07-31 00:00:00\n",
      "Total Revenue: $4,722,751.43\n",
      "Total Profit: $1,509,468.86\n",
      "Average Profit Margin: 30.25%\n",
      "\n",
      "üè≠ Product Line Breakdown:\n",
      "            Revenue                         Profit         ProfitMargin\n",
      "              count         sum    mean        sum    mean         mean\n",
      "ProductLine                                                            \n",
      "SCA            1964  1802690.11  917.87  569500.08  292.05        29.79\n",
      "SY             2529  2323684.14  918.82  752977.65  300.95        30.65\n",
      "TH              644   596377.18  926.05  186991.13  294.47        30.11\n",
      "\n",
      "‚öñÔ∏è Weight Class Distribution:\n",
      "            Revenue                         Profit         ProfitMargin\n",
      "              count         sum    mean        sum    mean         mean\n",
      "WeightClass                                                            \n",
      "Large          2121  1844921.60  869.84  491892.83  233.79        25.64\n",
      "Medium         1755  1647481.62  938.74  575070.02  331.07        33.20\n",
      "Small          1261  1230348.21  975.69  442506.01  355.14        33.97\n",
      "\n",
      "üè™ Store Performance:\n",
      "                           Revenue                         Profit          \\\n",
      "                             count         sum    mean        sum    mean   \n",
      "StoreID StoreName                                                           \n",
      "1       Atlas Equipment       1442  1300845.85  902.11  400348.49  280.55   \n",
      "2       Summit Machinery      1244  1132500.21  910.37  346692.29  282.32   \n",
      "3       Pioneer Industrial     769   723582.85  940.94  232753.01  305.85   \n",
      "4       Everest Heavy Duty     958   881429.85  920.07  327151.31  343.65   \n",
      "5       Liberty Machines       541   515387.21  952.66  154461.86  288.18   \n",
      "6       Titan Tools            183   169005.46  923.53   48061.90  262.63   \n",
      "\n",
      "                           ProfitMargin  \n",
      "                                   mean  \n",
      "StoreID StoreName                        \n",
      "1       Atlas Equipment           28.89  \n",
      "2       Summit Machinery          29.31  \n",
      "3       Pioneer Industrial        30.30  \n",
      "4       Everest Heavy Duty        34.83  \n",
      "5       Liberty Machines          27.95  \n",
      "6       Titan Tools               29.94  \n",
      "\n",
      "üó∫Ô∏è Top 10 States by Revenue:\n",
      "      Revenue                       Profit         ProfitMargin\n",
      "        count        sum    mean       sum    mean         mean\n",
      "State                                                          \n",
      "CA        348  306804.78  881.62  95384.94  276.48        29.62\n",
      "NY        293  271334.24  926.06  88013.80  303.50        30.13\n",
      "OH        253  230826.11  912.36  72833.37  290.17        31.56\n",
      "VA        219  209071.61  954.66  67712.53  310.61        30.49\n",
      "TX        213  191975.01  901.29  62736.17  294.54        29.15\n",
      "WA        187  172836.12  924.26  55089.90  296.18        29.71\n",
      "CT        169  159502.83  943.80  54663.51  327.33        31.61\n",
      "FL        175  158721.31  906.98  48451.67  278.46        29.40\n",
      "GA        169  154994.09  917.12  49641.02  295.48        29.23\n",
      "PA        164  149723.87  912.95  46966.55  286.38        29.06\n",
      "\n",
      "üë• Usage Category Performance:\n",
      "               Revenue                         Profit         ProfitMargin\n",
      "                 count         sum    mean        sum    mean         mean\n",
      "Usage                                                                     \n",
      "Civil              711   639442.41  899.36  200891.12  287.81        30.65\n",
      "Construction       336   324904.71  966.98  106450.48  319.67        31.25\n",
      "Education          634   578695.29  912.77  183210.12  291.27        29.68\n",
      "General           1351  1239739.83  917.65  387880.74  290.11        29.83\n",
      "Health             183   171215.79  935.61   55694.23  304.34        31.12\n",
      "Home               966   875876.95  906.70  277713.59  290.50        29.75\n",
      "Other              406   375747.21  925.49  127197.67  314.85        31.53\n",
      "Rental             444   422042.88  950.55  142538.88  321.76        30.59\n",
      "Transportation     106    95086.35  897.04   27892.03  268.19        29.86\n",
      "\n",
      "‚úÖ Cleaned data saved to: cleaned_BA_DummyData_2025.csv\n",
      "\n",
      "Data cleaning and analysis completed successfully!\n",
      "Final cleaned dataset shape: (6000, 20)\n",
      "description of cleaned data:\n",
      "            Weight                       SellDate                 ReceiveDate  \\\n",
      "count  6000.000000                           5137                        6000   \n",
      "mean    173.754667  2023-11-25 17:37:21.853221632  2023-09-25 19:59:45.600000   \n",
      "min      16.000000            2023-01-10 00:00:00         2022-09-03 00:00:00   \n",
      "25%      50.000000            2023-07-01 00:00:00         2023-04-27 00:00:00   \n",
      "50%     135.000000            2023-11-25 00:00:00         2023-09-14 12:00:00   \n",
      "75%     335.000000            2024-05-04 00:00:00         2024-03-06 00:00:00   \n",
      "max     500.000000            2024-07-31 00:00:00         2024-07-31 00:00:00   \n",
      "std     158.055478                            NaN                         NaN   \n",
      "\n",
      "       InventoryDays         Cost      Revenue       Profit  ProfitMargin  \\\n",
      "count    5137.000000  5950.000000  5137.000000  5087.000000   4996.000000   \n",
      "mean      102.033677   623.510179   919.359826   296.730659     30.252912   \n",
      "min         1.000000   200.800000     0.000000 -1262.800000    -72.894633   \n",
      "25%        38.000000   456.000000   623.000000   129.000000     20.894219   \n",
      "50%        76.000000   608.000000   861.300000   225.000000     26.242199   \n",
      "75%       139.000000   802.000000  1182.000000   415.850000     41.136925   \n",
      "max       495.000000  1578.528000  3025.100000  2317.900000     77.848347   \n",
      "std        89.064923   210.201186   410.732929   299.198165     14.467409   \n",
      "\n",
      "               ROI      StoreID  \n",
      "count  5087.000000  6000.000000  \n",
      "mean     48.274454     2.810167  \n",
      "min    -100.000000     1.000000  \n",
      "25%      25.977654     1.000000  \n",
      "50%      34.916201     3.000000  \n",
      "75%      69.174082     4.000000  \n",
      "max     351.433581     6.000000  \n",
      "std      43.673448     1.544623  \n"
     ]
    }
   ],
   "source": [
    "cleaned_data = main()\n",
    "if cleaned_data is not None:\n",
    "    print(\"\\nData cleaning and analysis completed successfully!\")\n",
    "    print(f\"Final cleaned dataset shape: {cleaned_data.shape}\")\n",
    "    print(\"description of cleaned data:\")\n",
    "    print(cleaned_data.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sany_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
